---
layout: post
title:  "A Deep Dive Into How R Fits a Linear Model"
date:   2016-06-03 08:52:55 -0700
categories: jekyll update
---

[R](https://www.r-project.org/) is a high level language for statistical computations.  One of my most used R functions is the humble `lm`, which fits a linear regression model.  The mathematics behind fitting a linear regression is relatively simple, some standard linear algebra with a touch of calculus.  It theredore may be quite suprising for the reader to learn that behind even the simplest of calls to R's `lm` function lies a journey through three different programming languages, which in the end arrives at some of the oldest open source code in existence.

So, in the spirit of the famous thought experiment "[what happens when you google something](https://github.com/alex/what-happens-when)", I'd like to discuss what heppens when you fit a linear model in R.

This post is inspired by a question of Antoni Parellada's on CrossValidated.  It is essentially a much expanded version of my answer there. 

We will make heavy use of the R source code, which you can find [here](https://github.com/wch/r-source).

The R Layer
-----------

Our point or orgin is `lm``, the interface exposed to the R programmer.  It offers a friendly way to specify models using the core R `formula` and `data.frame` datatypes.  A prototypical call to `lm` looks something like this

{% highlight r %}
m <- lm(y ~ x1 + x2, data = df)
{% endhighlight %}

The first argument is the model formula, and the second is a dataframe.  The dataframe must contain columns `x1`, `x2`, and `y`, whcih are used to fit the model.

The source code for and R function (except those implemented in the R source code itself, which are called `.Primitive`s) can be viewed by typing the function name into the R interpreter.  Typing `lm` reveals the full functon signature

{% highlight r %}
lm <- function (formula, data, subset, weights, na.action,
		method = "qr", model = TRUE, x = FALSE, y = FALSE,
		qr = TRUE, singular.ok = TRUE, contrasts = NULL,
		offset, ...)
{% endhighlight %}

It is worth a moment to point out, though after this point we will ignore it and focus on the interesting bits, that majority of the source code to `lm` (the same is true for the majority of most quality production level code) is boring but neccessary busy work and defensive programing: checking inputs, throwing errors

{% highlight r %}
...
if (!is.null(w) && !is.numeric(w)) 
    stop("'weights' must be a numeric vector")
...
{% endhighlight %}

and setting of object attributes

{% highlight r %}
...
z$na.action <- attr(mf, "na.action")
z$offset <- offset
z$contrasts <- attr(x, "contrasts")
z$xlevels <- .getXlevels(mt, mf)
...
{% endhighlight %}

Now, if we think at a high level, there are two fundamental tasks that `lm` must accomplish

  - It must consume the formula and dataframe it recieves and produce a design matrix $X$.
  - It must use this design matrix and some linear algebra to compute the linear regression coefficients.

both of these tasks turn out to, in fact, be interesting bits.


Constructing the Design Matrix
------------------------------

Obviously we need to construct a design matrix first.  This task starts in this dense and somewhat obscure block of code

{% highlight r %}
mf <- match.call(expand.dots = FALSE)
m <- match(c("formula", "data", "subset", "weights", "na.action", 
             "offset"), names(mf), 0L)
mf <- mf[c(1L, m)]
mf$drop.unused.levels <- TRUE
mf[[1L]] <- quote(stats::model.frame)
mf <- eval(mf, parent.frame())
{% endhighlight %}

This small block highlights a strange idiom of R programming, its direct manipulation of function calls, frozen in time.  The function `match.call`, when called upon in the local scope of another function, returns an object of type `call`, which captures the call to the enclosing function along with the values bound to its formal arguments.  That's pretty difficult to take in, so here is an example

{% highlight r %}
> f <- function(x, y) {
>   cl <- match.call()
>   cl
> }
> f(1, 2)
f(x = 1, y = 2)
> class(f(1, 2))
[1] "call"
{% endhighlight %}

A `call` object is a small wonder, it can be indexed into and manipulated much like other R objects.  The first index always gives the name of the function that was called (not as a string, as a [symbol](https://stat.ethz.ch/R-manual/R-devel/library/base/html/name.html) object) 

{% highlight r %}
> cl <- f(1, 2)
> cl[[1]]
f
> class(f(1, 2)[[1]])
[1] "name"
{% endhighlight %}

The other indicies return the arguments passed into the call

{% highlight r %}
> cl[[2]]
[1] 1
> cl[[3]]
[1] 2
{% endhighlight %}

In our current situation the function we called is `lm`, so the line

{% highlight r %}
mf[[1L]] <- quote(stats::model.frame)
{% endhighlight %}

replaces  the function name `lm` in the call object with `model.frame` (the `quote` creates a symbol out of a string).  Similarly, the lines

{% highlight r %}
m <- match(c("formula", "data", "subset", "weights", "na.action", 
             "offset"), names(mf), 0L)
mf <- mf[c(1L, m)]
mf$drop.unused.levels <- TRUE
{% endhighlight %}

discard any of the various arguments to `lm` that are not needed to construct the design matrix.  

Alltogether, we have gone from a call to `lm` like

{% highlight r %}
lm(y ~ x1 + x2, weights = w, data = df)
{% endhighlight %}

to a call to `model.frame` like

{% highlight r %}
model.frame(y ~ x1 + x2, weights = w, data = df)
{% endhighlight %}

Which is pretty neat.  We can now unstop time, and evaluate the function call we have so meticulously constructed

{% highlight r %}
mf <- eval(mf, parent.frame())
{% endhighlight %}

We get, courtesy of `model.frame`, a `data.frame` type object (that is, a `data.frame` with some additional attributes attached) that contains all the terms in our formula fully evaluated.  For example

{% highlight r %}
> df <- data.frame(
>      y = c(1, 2, 3, 4, 5),
>      x = c(5, 4, 3, 2, 1)
> )
> model.frame(y ~ log(x), data = df)
   y    log(x)
 1 1 1.6094379
 2 2 1.3862944
 3 3 1.0986123
 4 4 0.6931472
 5 5 0.0000000
{% endhighlight %}

Attached to `mf` is a [terms](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/terms.object.html) attribute, which contains metadata needed to identify which column represents the response, and which the predictors (the encoding of this information is pretty obscure, so I wont bother to unwind it here).  Using the data frame and the terms data, we can construct the final design matrix

{% highlight r %}
mt <- attr(mf, "terms")
x <- model.matrix(mt, mf, contrasts)
{% endhighlight %}

and response vector

###

Calculating the Regression - R
------------------------------

Now that we have a design matrix, we can move on to fitting the regression.

{% highlight r %}
lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...)
{% endhighlight %}

`lm.fit` is another R function, which we could call ourselives if we are so inclined

{% highlight r %}
> X = matrix(c(1, 1, 1, 1, 2, 3), nrow = 3)
> y = c(2, 3, 4)
> lm.fit(X, y)
 $coefficients
 x1 x2 
  1  1 
 ...
{% endhighlight %}

While `lm` conveniently works with formulas and `data.frame`s, `lm.fit` needs matrices, so moving from `lm` to `lm.fit` removes one layer of abstraction.  

Checking the source for `lm.fit`, there is much more busywork, which we will simply ignore.  The really interesting action is in this one line

{% highlight r %}
z <- .Call(C_Cdqrls, x, y, tol, FALSE)
{% endhighlight %}

Now we are getting somewhere.  `.Call` is modern R's way of calling into C code.  The first argument to `.Call` is a sting or symbol identifying a compiled C function that is either part of the R distribution, or linked as a shared library.  In some cases where the called function is part of the R source code, the function name is prepended with a `C_`. In our case, we are calling a C function named `Cdqrls`.  The remaining arguemnts to `.Call` are passed as R objects into the C function, as we will see shortly.


Calculating the Regression - C
------------------------------

The `Cdqrls` is found in the R source [here][1].  Note its peculiar signature

{% highlight c %}
SEXP Cdqrls(SEXP x, SEXP y, SEXP tol, SEXP chk)
{% endhighlight %}

A `SEXP` is the datatpye that R's source assigns to a general R object.  Essentailly everything that an R programmer manipulates when working day to day is internally an `SEXP`.  There are various subypes of `SEXP`s used for more specific types of objects, for example

  - A `INTSXP` is an integer vector.
  - A `REALSXP` is a vector of floating point numbers.
  - A `VECSXP` is an R list.

More advanced types are also `SEXP`s, a `CLOSXP` is an R function.

Knowing these types, we can make some sense of the code in `Cdqrls`.  Here, a list is created to hold the output object

{% highlight c %}
const char *ansNms[] = {"qr", "coefficients", "residuals", "effects",
			    "rank", "pivot", "qraux", "tol", "pivoted", ""};
PROTECT(ans = mkNamed(VECSXP, ansNms));
{% endhighlight %}

Here, a vector of floating point numbers is created to hold the fit coefficints, which is then inserted in the appropriate slot in the ouput object

{% highlight c %}
coefficients = (ny > 1) ? allocMatrix(REALSXP, p, ny) : allocVector(REALSXP, p);
PROTECT(coefficients);
SET_VECTOR_ELT(ans, 1, coefficients);
{% endhighlight %} 

notice that, depending on the shape of `y`, `coefficients` can be either a matrix or a regular vector.

The `PROTECT` macro issues instruction to Rs garbage collector, new objects must be `PROTECTE`D, lest they be collected.

Yet again, the majority of the work in `Cdqrls` is concerned with checking invariants of inputs, and constructing and initilizing new objects.  And, once more, the real work of fitting the model is passed to another function, further, we once again pass to another language 

{% highlight c %}
    F77_CALL(dqrls)(REAL(qr), &n, &p, REAL(y), &ny, &rtol,
		    REAL(coefficients), REAL(residuals), REAL(effects),
		    &rank, INTEGER(pivot), REAL(qraux), work);
{% endhighlight %}


Calculating the Regression - FORTRAN
------------------------------------

So now we are on our third language, R has called C which is calling into fortran.  [Here's the fortran code][2].

The first comment tells it all

{% highlight fortran %}
    c     dqrfit is a subroutine to compute least squares solutions
    c     to the system
    c
    c     (1)               x * b = y
{% endhighlight %}

(interestingly, looks like the name of this routine was changed at some point, but someone forgot to update the comment).  So we're finally at the point where we can do some linear algebra, and actually solve the system of equations.  This is the sort of thing that fortran is really good at, which explains why we passed through so many layers to get here. 

The comment also explains what the code is going to do

{% highlight fortran %}
    c     on return
    c
    c        x      contains the output array from dqrdc2.
    c               namely the qr decomposition of x stored in
    c               compact form.
{% endhighlight %}

So fortran is going to solve the system by finding the $QR$ decomposition.

The first thing that happens, and by far the most important, is

{% highlight fortran %}
    call dqrdc2(x,n,n,p,tol,k,qraux,jpvt,work)
{% endhighlight %}

This calls the fortran function `dqrdc2` on our input matrix `x`.  Whats this?  

{% highlight fortran %}
     c     dqrfit uses the linpack routines dqrdc and dqrsl.
{% endhighlight %}

So we've finally made it to [linpack][3].  Linpack is a fortran linear algebra library that has been around since the 70s.  Most serious linear algebra eventualy finds its way to linpack.  In our case, we are using the function [dqrdc2][4]

{% highlight fortran %}
    c     dqrdc2 uses householder transformations to compute the qr
    c     factorization of an n by p matrix x.
{% endhighlight %}

This is where the actual work is done.  It would take a good full day for me to figure out what this code is doing, it is as low level as they come.  But generically, we have a matrix $X$ and we want to factor it into a product $X = QR$ where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix.  This is a smart thing to do, because once you have $Q$ and $R$ you can solve the linear equations for regression

$$ X^t X \beta = X^t Y $$

very easily.  Indeed

$$ X^t X = R^t Q^t Q R = R^t R $$

so the whole system becomes

$$ R^t R \beta = R^t Q^t y $$

but $R$ is upper triangular and has the same rank as $X^t X$, so as long as our problem is well posed, it is full rank, and we may as well just solve the reduced system

$$ R \beta = Q^t y $$

But here's the awesome thing.  $R$ is upper triangular, so the last linear equation here is just `constant * beta_n = constant`, so solving for $\beta_n$ is trivial.  You can then go up the rows, one by one, and substitute in the $\beta$s you already know, each time getting a simple one variable linear equation to solve.  So, once you have $Q$ and $R$, the whole thing collapses to what is called *backwards substitution*, which is easy.  You can read about this in more detail [here][5], where an explicit small example is fully worked out.


  [1]: https://github.com/wch/r-source/blob/trunk/src/library/stats/src/lm.c
  [2]: https://github.com/wch/r-source/blob/trunk/src/appl/dqrls.f
  [3]: http://en.wikipedia.org/wiki/LINPACK
  [4]: https://github.com/wch/r-source/blob/trunk/src/appl/dqrdc2.f
  [5]: http://www.seas.ucla.edu/~vandenbe/103/lectures/qr.pdf

{% highlight ruby %}
def print_hi(name)
  puts "Hi, #{name}"
end
print_hi('Tom')
#=> prints 'Hi, Tom' to STDOUT.
{% endhighlight %}

[jekyll-docs]: http://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
